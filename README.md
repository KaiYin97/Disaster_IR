# DisastIR
The source code for **DisastIR: A Comprehensive Information Retrieval Benchmark for Disaster Management.**
We develop this complete end-to-end pipeline for constructing Disaster Information Retrieval (DisastIR) benchmark from scratch, starting from PDF download and text extraction, through semantic chunking and deduplication, to user query generation, labeling-pool construction, and final relevance scoring.

---

## ğŸ” Project Overview

1. **Corpus Construction**  
   - Download disaster management-related PDFs.  
   - Extract and clean text.  
   - Deduplicate at the file and chunk level.  
   - Semantic chunking into passages.

2. **Query Generation**  
   - Passage-level tasks: Twitter, QA, STS, Fact-Check, NLI.  
   - Document-level QA.

3. **Retrieval & Label Pool**  
   - Encode corpus & queries with dense models.  
   - Build ANN indices, perform exact & approximate retrieval.  
   - Merge top-K results into `label_pool.json`.

4. **Relevance Scoring**  
   - Rate each (query, passage) pair via three LLM-based raters:  
     - Four-phase decomposed  
     - Chain-of-Thought  
     - Zero-Shot  
   - Aggregate scores (majority vote, average).

---

## ğŸ“‚ Directory Structure

```
DisastIR/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ path_config.py      # all paths
â”‚   â”œâ”€â”€ model_config.py     # model names, device, batch sizes
â”‚   â””â”€â”€ gen_config.py       # LLM settings, prompt templates
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw_pdf/
â”‚   â”œâ”€â”€ txt_cleaned/
â”‚   â”œâ”€â”€ deduped_chunks/
â”‚   â”œâ”€â”€ embeddings/
â”‚   â””â”€â”€ label_pools/
â”œâ”€â”€ outputs/
â”‚   â”œâ”€â”€ scores/
â”‚   â””â”€â”€ results/
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ build_corpus.py
â”‚   â”œâ”€â”€ generate_queries.py
â”‚   â”œâ”€â”€ build_label_pool.py
â”‚   â””â”€â”€ relevance_scoring.py
â””â”€â”€ src/
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ corpus/             # downloader, manager, processor, chunker, deduper
    â”œâ”€â”€ query/              # client, generators, pipeline
    â”œâ”€â”€ retrieval/          # embedder, index_builder, retriever, label_pool_builder
    â”œâ”€â”€ scoring/            # scorer + raters
    â””â”€â”€ utils/              # embedding, llm, io, misc helpers
```

---

## ğŸ› ï¸ Installation

1. **Clone & enter project**  
   ```bash
   git clone https://<your-repo>.git
   cd DisastIR
   ```

2. **Create a Python 3.10+ virtual env**  
   ```bash
   python -m venv .venv
   source .venv/bin/activate
   ```

3. **Install dependencies**  
   ```bash
   pip install -r requirements.txt
   ```

4. **Set your OpenAI API key**  
   Edit `configs/gen_config.py`:
   ```python
   OPENAI_API_KEY = "sk-..."
   ```

---

## âš™ï¸ Configuration

All paths and constants are centralized:

- **`configs/path_config.py`**  
- **`configs/model_config.py`**  
- **`configs/gen_config.py`**

---

## ğŸš€ Pipeline Steps

### 1. Build Corpus

```bash
python scripts/build_corpus.py \
  --keywords_json path/to/keywords.json
```

### 2. Generate Queries

```bash
python scripts/generate_queries.py \
  --tasks all \
  --filename chunks_001.json \
  --start_index 0 \
  --end_index 100
```

### 3. Build Label Pool

```bash
python scripts/build_label_pool.py
```

### 4. Relevance Scoring

```bash
python scripts/relevance_scoring.py \
  --task QA \
  --file_index 0 \
  --part_index 0 \
  --input_dir data/label_pools \
  --output_dir outputs/scores \
  --force_default_prompts
```

---

## ğŸ“ˆ Viewing Results

- **Raw scores**: `outputs/scores/<task>_â€¦_qrels.json`  
- **Reports**: under `outputs/results`.

---

## ğŸ“ Tips

- Adjust `configs/*_config.py` for batch sizes, retrials.  
- Run in parallel by splitting file_index/part_index.  
- Monitor OpenAI usage for quotas.


